{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from time import time\n",
    "import datetime\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import re\n",
    "from collections import Counter\n",
    "from nltk.corpus import words\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from transformers import (\n",
    "                BertConfig, BertForSequenceClassification, BertTokenizer,\n",
    "              XLNetConfig, XLNetForSequenceClassification, XLNetTokenizer)\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda 0: Tesla V100-PCIE-32GB\n",
      "cuda 1: Tesla M40\n",
      "cuda 2: Tesla M40\n",
      "cuda 3: GeForce GTX 1080\n",
      "cuda 4: GeForce GTX 1080\n",
      "cuda 5: Tesla V100-PCIE-32GB\n",
      "cuda 6: Tesla P100-PCIE-16GB\n",
      "cuda 7: Tesla P100-PCIE-16GB\n"
     ]
    }
   ],
   "source": [
    "# print gpu devices\n",
    "if torch.cuda.device_count() > 0:\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print('cuda {}: {}'.format(i, torch.cuda.get_device_name(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you are now using Tesla V100-PCIE-32GB.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('you are now using {}.'.format(torch.cuda.get_device_name(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed value all over the place to make this reproducible.\n",
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "    np.random.seed(seed)  # Numpy module.\n",
    "    random.seed(seed)  # Python random module.\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True  \n",
    "\n",
    "setup_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {}\n",
    "CONFIG['A_train_path'] = \"datasets/train/SemEval2018-T3-train-taskA_emoji.txt\"\n",
    "CONFIG['A_test_path'] = \"datasets/goldtest_TaskA/SemEval2018-T3_gold_test_taskA_emoji.txt\"\n",
    "CONFIG['B_train_path'] = \"datasets/train/SemEval2018-T3-train-taskB_emoji.txt\"\n",
    "CONFIG['B_test_path'] = \"datasets/goldtest_TaskB/SemEval2018-T3_gold_test_taskB_emoji.txt\"\n",
    "CONFIG['bert_epochs'] = 4\n",
    "CONFIG['xlnet_epochs'] = 4\n",
    "CONFIG['batch_size'] = 32\n",
    "CONFIG['max_len'] = 128\n",
    "CONFIG['bert_models'] = ['bert-base-uncased', 'bert-base-cased', 'bert-large-uncased', 'bert-large-cased']\n",
    "CONFIG['xlnet_models'] = ['xlnet-base-cased', 'xlnet-large-cased']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# some util functions, just run it\n",
    "\n",
    "# get score\n",
    "def print_score(true, predicted, task='A'):\n",
    "    acc = calc_accuracy(true, predicted)\n",
    "    if task == \"A\":\n",
    "        p, r, f = precision_recall_fscore(true, predicted, beta=1, labels=[0,1], pos_label=1)\n",
    "    elif task == \"B\":\n",
    "        p, r, f = precision_recall_fscore(true, predicted, beta=1, labels=[0,1,2,3])\n",
    "    print(\"Accuracy:{0}\\nPrecision:{1}\\nRecall:{2}\\nF1-score:{3}\\n\".format(acc, p,r,f))\n",
    "            \n",
    "\n",
    "def calc_accuracy(true, predicted):\n",
    "    \"\"\"Calculates the accuracy of a (multiclass) classifier, defined as the fraction of correct classifications.\"\"\"\n",
    "    return sum([t==p for t,p in zip(true, predicted)]) / float(len(true))\n",
    "\n",
    "\n",
    "def precision_recall_fscore(true, predicted, beta=1, labels=None, pos_label=None, average=None, each=None):\n",
    "    \"\"\"Calculates the precision, recall and F-score of a classifier.\n",
    "    :param true: iterable of the true class labels\n",
    "    :param predicted: iterable of the predicted labels\n",
    "    :param beta: the beta value for F-score calculation\n",
    "    :param labels: iterable containing the possible class labels\n",
    "    :param pos_label: the positive label (i.e. 1 label for binary classification)\n",
    "    :param average: selects weighted, micro- or macro-averaged F-score\n",
    "    \"\"\"\n",
    "\n",
    "    # Build contingency table as ldict\n",
    "    ldict = {}\n",
    "    for l in labels:\n",
    "        ldict[l] = {\"tp\": 0., \"fp\": 0., \"fn\": 0., \"support\": 0.}\n",
    "\n",
    "    for t, p in zip(true, predicted):\n",
    "        if t == p:\n",
    "            ldict[t][\"tp\"] += 1\n",
    "        else:\n",
    "            ldict[t][\"fn\"] += 1\n",
    "            ldict[p][\"fp\"] += 1\n",
    "        ldict[t][\"support\"] += 1\n",
    "\n",
    "    # Calculate precision, recall and F-beta score per class\n",
    "    beta2 = beta ** 2\n",
    "    for l, d in ldict.items():\n",
    "        try:\n",
    "            ldict[l][\"precision\"] = d[\"tp\"]/(d[\"tp\"] + d[\"fp\"])\n",
    "        except ZeroDivisionError: ldict[l][\"precision\"] = 0.0\n",
    "        try: ldict[l][\"recall\"]    = d[\"tp\"]/(d[\"tp\"] + d[\"fn\"])\n",
    "        except ZeroDivisionError: ldict[l][\"recall\"]    = 0.0\n",
    "        try: ldict[l][\"fscore\"] = (1 + beta2) * (ldict[l][\"precision\"] * ldict[l][\"recall\"]) / (beta2 * ldict[l][\"precision\"] + ldict[l][\"recall\"])\n",
    "        except ZeroDivisionError: ldict[l][\"fscore\"] = 0.0\n",
    "\n",
    "            \n",
    "    if each:\n",
    "        return [ldict[l][\"fscore\"] for l in labels]\n",
    "    # If there is only 1 label of interest, return the scores. No averaging needs to be done.\n",
    "    if pos_label:\n",
    "        d = ldict[pos_label]\n",
    "        return (d[\"precision\"], d[\"recall\"], d[\"fscore\"])\n",
    "    # If there are multiple labels of interest, macro-average scores.\n",
    "    else:\n",
    "        for label in ldict.keys():\n",
    "            avg_precision = sum(l[\"precision\"] for l in ldict.values()) / len(ldict)\n",
    "            avg_recall = sum(l[\"recall\"] for l in ldict.values()) / len(ldict)\n",
    "            avg_fscore = sum(l[\"fscore\"] for l in ldict.values()) / len(ldict)\n",
    "        return (avg_precision, avg_recall, avg_fscore)\n",
    "\n",
    "# get ids and masks\n",
    "def get_ids_mask(sents, tokenizer, max_len=None):\n",
    "    t_e = [tokenizer.encode_plus(sent, \n",
    "                              max_length = max_len,\n",
    "                              add_special_tokens = True,\n",
    "                              pad_to_max_length = 'right',\n",
    "#                                 return_tensors='pt',\n",
    "                             ) for sent in sents]\n",
    "    \n",
    "    input_ids, attention_masks = [], []    \n",
    "    \n",
    "    for x in t_e:\n",
    "        input_ids.append(x['input_ids'])\n",
    "        attention_masks.append(x['attention_mask'])\n",
    "    \n",
    "    return input_ids, attention_masks\n",
    "\n",
    "# format time for training time\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tuning function\n",
    "def ft(task='A', model_type='BERT', name='bert-base-uncased', verbose=False, each=None):\n",
    "    df = pd.read_csv(CONFIG[task+'_train_path'], delimiter='\\t', index_col=0)\n",
    "    df_test = pd.read_csv(CONFIG[task+'_test_path'], delimiter='\\t', index_col=0)\n",
    "    \n",
    "    print('Training Dataset has {} sentences.'.format(df.shape[0]))\n",
    "    print('Test Dataset has {} sentences.'.format(df_test.shape[0]))\n",
    "\n",
    "    \n",
    "    # bert and xlnet have different format\n",
    "    # data preprocessing\n",
    "    with open('normalized_sents.pickle', 'rb') as f:\n",
    "        tv_sents, test_sents = pickle.load(f) \n",
    "    tv_labels = df['Label'].values\n",
    "    test_labels = df_test['Label'].values\n",
    "    \n",
    "    #initialize tokenizer\n",
    "    if model_type == 'BERT':\n",
    "        if 'uncased' in name:\n",
    "            tokenizer =  BertTokenizer.from_pretrained(name, do_lower_case=True)\n",
    "        else:\n",
    "            tokenizer =  BertTokenizer.from_pretrained(name)\n",
    "    elif model_type == 'XLNet':\n",
    "        tokenizer = XLNetTokenizer.from_pretrained(name)\n",
    "    \n",
    "    #get tokens for training\n",
    "    tv_ids, tv_masks = get_ids_mask(tv_sents, tokenizer, max_len=CONFIG['max_len'])\n",
    "    test_ids, test_masks = get_ids_mask(test_sents, tokenizer, max_len=CONFIG['max_len'])\n",
    "\n",
    "    # Use 90% for training and 10% for validation.\n",
    "    train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(tv_ids, tv_labels, random_state=42, test_size=0.1)\n",
    "    # Do the same for the masks.\n",
    "    train_masks, validation_masks, _, _ = train_test_split(tv_masks, tv_labels, random_state=42, test_size=0.1)\n",
    "\n",
    "    train_inputs = torch.tensor(train_inputs)\n",
    "    validation_inputs = torch.tensor(validation_inputs)\n",
    "\n",
    "    train_labels = torch.tensor(train_labels)\n",
    "    validation_labels = torch.tensor(validation_labels)\n",
    "\n",
    "    train_masks = torch.tensor(train_masks)\n",
    "    validation_masks = torch.tensor(validation_masks)\n",
    "\n",
    "    prediction_inputs = torch.tensor(test_ids)\n",
    "    prediction_masks = torch.tensor(test_masks)\n",
    "    prediction_labels = torch.tensor(test_labels)\n",
    "    prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
    "    prediction_dataloader = DataLoader(prediction_data,\n",
    "                    shuffle=True, batch_size=CONFIG['batch_size'])\n",
    "\n",
    "    # Create the DataLoader for our training set.\n",
    "    train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "    train_dataloader = DataLoader(train_data, \n",
    "                    shuffle=True, batch_size=CONFIG['batch_size'])\n",
    "\n",
    "    # Create the DataLoader for our validation set.\n",
    "    validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "    validation_dataloader = DataLoader(validation_data, \n",
    "                    shuffle=True, batch_size=CONFIG['batch_size'])\n",
    "\n",
    "    # initialize model\n",
    "    num_labels = 2 if task=='A' else 4\n",
    "    if model_type == 'BERT':\n",
    "        model = BertForSequenceClassification.from_pretrained(name, num_labels=num_labels)\n",
    "    elif model_type == 'XLNet':\n",
    "        model = XLNetForSequenceClassification.from_pretrained(name, num_labels=num_labels)\n",
    "\n",
    "    # Tell pytorch to run this model on the GPU. \n",
    "#     if torch.cuda.device_count() > 1:\n",
    "#         # data parallelism\n",
    "#         model = torch.nn.DataParallel(model)\n",
    "#         print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model.to(device)\n",
    "\n",
    "    #initialize optimizer\n",
    "    if model_type == 'BERT':\n",
    "        epochs = CONFIG['bert_epochs']\n",
    "        optimizer = AdamW(model.parameters(),\n",
    "              lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "              eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "            )\n",
    "        # Total number of training steps is number of batches * number of epochs.\n",
    "        total_steps = len(train_dataloader) * epochs\n",
    "        # total_steps = len(all_dataloader) * CONFIG['epochs']\n",
    "\n",
    "        # Create the learning rate scheduler.\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                                    num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                                    num_training_steps = total_steps)\n",
    "\n",
    "    elif model_type == 'XLNet':\n",
    "        param_optimizer = list(model.named_parameters())\n",
    "        no_decay = ['bias', 'gamma', 'beta']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "             'weight_decay_rate': 0.01},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "             'weight_decay_rate': 0.0}\n",
    "        ]\n",
    "\n",
    "        optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                          lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                        )\n",
    "        epochs = CONFIG['xlnet_epochs']\n",
    "\n",
    "    # training\n",
    "\n",
    "    # Store the average loss after each epoch so we can plot them.\n",
    "    loss_values = []\n",
    "\n",
    "    model.zero_grad()\n",
    "\n",
    "    # For each epoch...\n",
    "    for epoch_i in range(0, epochs):\n",
    "\n",
    "        # ========================================\n",
    "        #               Training\n",
    "        # ========================================\n",
    "\n",
    "        # Perform one full pass over the training set.\n",
    "        if verbose:\n",
    "            print(\"\")\n",
    "            print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "            print('Training...')\n",
    "\n",
    "        # Measure how long the training epoch takes.\n",
    "        t0 = time()\n",
    "\n",
    "        # Reset the total loss for this epoch.\n",
    "        total_loss = 0\n",
    "\n",
    "        # Set our model to training mode (as opposed to evaluation mode)\n",
    "        model.train()\n",
    "\n",
    "        # This training code is based on the `run_glue.py` script here:\n",
    "        # https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "        # For each batch of training data...\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "            # Progress update every 40 batches.\n",
    "            if step % 40 == 0 and not step == 0:\n",
    "                # Calculate elapsed time in minutes.\n",
    "                elapsed = format_time(time() - t0)\n",
    "\n",
    "                # Report progress.\n",
    "                if verbose:\n",
    "                    print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "\n",
    "            # Unpack this training batch from our dataloader. \n",
    "            #\n",
    "            # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "            # `to` method.\n",
    "            #\n",
    "            # `batch` contains three pytorch tensors:\n",
    "            #   [0]: input ids \n",
    "            #   [1]: attention masks\n",
    "            #   [2]: labels \n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "\n",
    "            # Forward pass (evaluate the model on this training batch)\n",
    "            # `model` is of type: pytorch_pretrained_bert.modeling.BertForSequenceClassification\n",
    "            outputs = model(b_input_ids, \n",
    "                        token_type_ids=None, \n",
    "                        attention_mask=b_input_mask, \n",
    "                        labels=b_labels)\n",
    "\n",
    "            loss = outputs[0]\n",
    "\n",
    "            # Accumulate the loss. `loss` is a Tensor containing a single value; \n",
    "            # the `.item()` function just returns the Python value from the tensor.\n",
    "#             total_loss += loss.mean().item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Perform a backward pass to calculate the gradients.\n",
    "#             loss.mean().backward()\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0.\n",
    "            if model_type == 'BERT':\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # Update parameters and take a step using the computed gradient\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update the learning rate.\n",
    "            if model_type == 'BERT':\n",
    "                scheduler.step()\n",
    "\n",
    "            # Clear out the gradients (by default they accumulate)\n",
    "            model.zero_grad()\n",
    "\n",
    "        # Calculate the average loss over the training data.\n",
    "        avg_train_loss = total_loss / len(train_dataloader)            \n",
    "\n",
    "        loss_values.append(avg_train_loss)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"\")\n",
    "            print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "            print(\"  Training epcoh took: {:}\".format(time() - t0))\n",
    "\n",
    "        # ========================================\n",
    "        #               Validation\n",
    "        # ========================================\n",
    "        # After the completion of each training epoch, measure our performance on\n",
    "        # our validation set.\n",
    "\n",
    "            print(\"\")\n",
    "            print(\"Running Validation...\")\n",
    "\n",
    "        t0 = time()\n",
    "\n",
    "        # Put model in evaluation mode to evaluate loss on the validation set\n",
    "        model.eval()\n",
    "\n",
    "        # Tracking variables \n",
    "        preds, labels = [], []\n",
    "\n",
    "        # Evaluate data for one epoch\n",
    "        for batch in validation_dataloader:\n",
    "\n",
    "            # Add batch to GPU\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            # Unpack the inputs from our dataloader\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "            # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "            with torch.no_grad():        \n",
    "                # Forward pass, calculate logit predictions\n",
    "                # token_type_ids is for the segment ids, but we only have a single sentence here.\n",
    "                # See https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L258 \n",
    "                outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "\n",
    "            logits = outputs[0]\n",
    "\n",
    "            # Move logits and labels to CPU\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "            preds.append(np.argmax(logits, axis=1))\n",
    "            labels.append(label_ids)\n",
    "\n",
    "\n",
    "        # Report the final accuracy for this validation run.\n",
    "        preds = [item for sublist in preds for item in sublist]\n",
    "        labels = [item for sublist in labels for item in sublist]\n",
    "        # print(preds, labels)\n",
    "\n",
    "        if verbose:\n",
    "            print_score(preds, labels, task)\n",
    "            print(\"  Validation took: {:}\".format(format_time(time() - t0)))\n",
    "    if verbose:\n",
    "        print(\"\")\n",
    "        print(\"Training complete!\")\n",
    "\n",
    "        # show the training loss figure\n",
    "        plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "        # Plot the learning curve.\n",
    "        plt.plot(loss_values, 'b-o')\n",
    "\n",
    "        # Label the plot.\n",
    "        plt.title(\"Training loss\")\n",
    "        plt.xlabel(\"Batch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        # Prediction on test set\n",
    "        print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
    "\n",
    "    # Put model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    t0 = time()\n",
    "\n",
    "    # Tracking variables \n",
    "    predictions , true_labels = [], []\n",
    "\n",
    "    # Predict \n",
    "    for batch in prediction_dataloader:\n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "        # Telling the model not to compute or store gradients, saving memory and \n",
    "        # speeding up prediction\n",
    "        with torch.no_grad():\n",
    "          # Forward pass, calculate logit predictions\n",
    "          outputs = model(b_input_ids, token_type_ids=None, \n",
    "                          attention_mask=b_input_mask)\n",
    "\n",
    "        logits = outputs[0]\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Store predictions and true labels\n",
    "        predictions.append(np.argmax(logits, axis=1))\n",
    "        true_labels.append(label_ids)\n",
    "\n",
    "    predictions = [item for sublist in predictions for item in sublist]\n",
    "    true_labels = [item for sublist in true_labels for item in sublist]\n",
    "    print(\"Epochs: {}, batch size: {}\".format(epochs, CONFIG['batch_size']))\n",
    "    print(\"The {} model's result for task {} is:\".format(name, task))\n",
    "    print_score(true_labels, predictions, task)\n",
    "    if each == True:\n",
    "        print(precision_recall_fscore(true_labels, predictions, beta=1, labels=[0,1,2,3], each=True))\n",
    "    if verbose:\n",
    "        print(\"  Prediction took: {:}\".format(format_time(time() - t0)))\n",
    "        print('    DONE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset has 3817 sentences.\n",
      "Test Dataset has 784 sentences.\n",
      "Epochs: 4, batch size: 32\n",
      "The bert-large-uncased model's result for task B is:\n",
      "Accuracy:0.6645408163265306\n",
      "Precision:0.44743937791473576\n",
      "Recall:0.4685547165290084\n",
      "F1-score:0.4449276661884266\n",
      "\n",
      "[0.7583333333333334, 0.6390243902439025, 0.38235294117647056, 0.0]\n"
     ]
    }
   ],
   "source": [
    "ft(task='B', model_type='BERT', name='bert-large-uncased', verbose=False, each=True)\n",
    "# ft(task='B', model_type='XLNet', name='xlnet-base-cased', verbose=False, each=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
